[{"section":"Blog","slug":"/blog/post-1/","title":"Introduction to Langchain","description":"A brief intro of Langchain","date":"August 2, 2023","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/Blog/Langchain_hu783282be189266eab9fd4910bf4624cb_48468_420x0_resize_q90_h2_lanczos_3.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/Blog\\/Langchain_hu783282be189266eab9fd4910bf4624cb_48468_420x0_resize_lanczos_3.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/Blog/Langchain_hu783282be189266eab9fd4910bf4624cb_48468_100x100_fill_q90_h2_lanczos_smart1_3.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/Blog\\/Langchain_hu783282be189266eab9fd4910bf4624cb_48468_100x100_fill_lanczos_smart1_3.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"NLP, LLM","tags":"Langchain, Deep Lake","content":"LangChain Introducing LangChain LangChain: A cutting-edge open-source framework designed to amplify the capabilities of your LLM (Language Learning Model).\nCore Features of LangChain Prompt Management: Efficiently manage and optimize prompts tailored for diverse tasks. Task Subdivision: Seamlessly transition between various subtasks within a primary task. Data Augmentation Generation: This involves a unique chaining process that collaborates with external data sources to fetch data for the generation phase. For instance, generating summaries from extensive texts or Q\u0026amp;A sessions based on specific data sources. Agent Operations: Agents execute distinct actions based on varying instructions until the entire operation concludes. Evaluation Mechanism: Traditional metrics often fall short when evaluating generative models. LangChain introduces an innovative approach by utilizing the language model itself for evaluations, offering supportive hints and chains. Memory Management: Efficiently handle intermediate states throughout the process. To encapsulate, LangChain can be visualized as a system that manages and refines prompts throughout a process\u0026rsquo;s lifecycle. It employs various agents to execute actions based on these prompts, utilizes memory to oversee intermediate states, and then harnesses chains to interlink different agents, forming a cohesive loop.\nValue Propositions of LangChain Components: These are abstract constructs designed for language model processing, accompanied by a suite of implementations for each concept. Regardless of your engagement with other LangChain features, these components are modular and user-friendly. Pre-configured Chains: These are structured amalgamations of components aimed at executing specific high-tier tasks. While these chains offer a quick start, the components ensure easy customization for intricate applications. Key Components of LangChain Model I/O: Interface for the language model. Data Connection: Interface tailored for specific tasks. Chains: Construct a series of calls. Agents: Upon receiving high-level directives, the chain decides the appropriate tools. Memory: Retain application state between chain runs. Callbacks: Document and relay any intermediate chain steps. Indexes: Techniques to structure documents for optimal LLM interaction. Data Connection: A Deep Dive LLM applications often necessitate user-specific data that isn\u0026rsquo;t part of the model\u0026rsquo;s foundational training set. LangChain offers a comprehensive toolkit for data handling:\nDocument Loader: Fetch documents from a plethora of sources. Document Converter: Segment documents, eliminate superfluous ones, and more. Text Embedding Model: Convert unstructured text into float lists. Vector Storage: Archive and search embedded data. Retriever: Efficiently search through your data. Visual Representation of Data Connection Setting Up Data Connection: Document Loader Python installation package command:\npip install langchain pip install unstructured pip install jq CSV basic usage\nimport os from pathlib import Path from langchain.document_loaders import UnstructuredCSVLoader _ from langchain.document_loaders.csv_loader import CSVLoader EXAMPLE_DIRECTORY = file_path = Path(__file__ ) .parent .parent / \u0026#34;examples\u0026#34; def test_unstructured_csv_loader ( ) - \u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Test unstructured loader.\u0026#34;\u0026#34;\u0026#34; file_path = os.path.join ( EXAMPLE_DIRECTORY , \u0026#34;stanley-cups.csv\u0026#34;) loader = UnstructuredCSVLoader (str( file_path )) docs = loader.load () print(docs) assert len (docs) == 1 def test_csv_loader ( ) : file_path = os.path.join ( EXAMPLE_DIRECTORY , \u0026#34;stanley-cups.csv\u0026#34;) loader = CSVLoader ( file_path ) docs = loader.load () print(docs) test_unstructured_csv_loader ( ) _ test_csv_loader ( ) _ File directory usage\nfrom langchain.document_loaders import DirectoryLoader , TextLoader _ text_loader_kwargs = { \u0026#39; autodetect_encoding \u0026#39;: True} loader = DirectoryLoader ( \u0026#39;../examples/\u0026#39;, glob=\u0026#34;**/*.txt\u0026#34;, # traverse txt files show_progress =True, # show progress use_multithreading =True, # use multithreading loader_cls = TextLoader , # use the way to load data silent_errors =True, #Continue when encountering an error loader_kwargs = text_loader_kwargs ) # You can use a dictionary to pass in parameters docs = loader.load () print(\u0026#34;\\n\u0026#34;) print( docs[ 0]) HTML usage\nfrom langchain.document_loaders import UnstructuredHTMLLoader , BSHTMLLoader _ loader = UnstructuredHTMLLoader (\u0026#34;../examples/example.html\u0026#34;) docs = loader.load () print( docs[ 0]) loader = BSHTMLLoader (\u0026#34;../examples/example.html\u0026#34;) docs = loader.load () print( docs[ 0]) JSON Usage\nimport json from pathlib import Path from pprint import pprint file_path = \u0026#39;../examples/facebook_chat.json \u0026#39; data = json. loads (Path( file_path ). read_text ()) pprint (data) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;image\u0026#39;: {\u0026#39; creation_timestamp \u0026#39;: 1675549016, \u0026#39; uri \u0026#39;: \u0026#39;image_of_the_chat.jpg\u0026#39;}, \u0026#39; is_still_participant \u0026#39;: True , \u0026#39; joinable_mode \u0026#39;: {\u0026#39;link\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;mode\u0026#39;: 1} , \u0026#39; magic_words \u0026#39;: [] , \u0026#39;messages\u0026#39;: [{\u0026#39;content\u0026#39;: \u0026#39;Bye!\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 2\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675597571851}, {\u0026#39;content\u0026#39;: \u0026#39;Oh no worries! Bye\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 1\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675597435669}, {\u0026#39;content\u0026#39;: \u0026#39;No Im sorry it was my mistake, the blue one is not \u0026#39; \u0026#39; for sale\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 2\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675596277579}, {\u0026#39;content\u0026#39;: \u0026#39;I thought you were selling the blue one!\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 1\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675595140251}, {\u0026#39;content\u0026#39;: \u0026#39; Im not interested in this bag. Im interested in the \u0026#39; \u0026#39; blue one!\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 1\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675595109305}, {\u0026#39;content\u0026#39;: \u0026#39;Here is $129\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 2\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675595068468}, {\u0026#39;photos\u0026#39;: [{\u0026#39; creation_timestamp \u0026#39;: 1675595059, \u0026#39; uri \u0026#39;: \u0026#39;url_of_some_picture.jpg\u0026#39;}], \u0026#39; sender_name \u0026#39;: \u0026#39;User 2\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675595060730}, {\u0026#39;content\u0026#39;: \u0026#39;Online is at least $100\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 2\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675595045152}, {\u0026#39;content\u0026#39;: \u0026#39;How much do you want?\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 1\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675594799696}, {\u0026#39;content\u0026#39;: \u0026#39; Goodmorning ! $50 is too low.\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 2\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675577876645}, {\u0026#39;content\u0026#39;: \u0026#39;Hi! Im interested in your bag. Im offering $50. Let \u0026#39; \u0026#39; I know if you are interested. Thanks!\u0026#39;, \u0026#39; sender_name \u0026#39;: \u0026#39;User 1\u0026#39; , \u0026#39; timestamp _ms \u0026#39;: 1675549022673}], \u0026#39;participants\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;User 1\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;User 2\u0026#39;}], \u0026#39; thread _path \u0026#39;: \u0026#39;inbox/User 1 and User 2 chat\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;User 1 and User 2 chat\u0026#39;} \u0026#34;\u0026#34;\u0026#34; Load data using langchain : ```python from langchain.document_loaders import JSONLoader _ loader = JSONLoader ( file_path =\u0026#39;../examples/ facebook_chat.json \u0026#39;, jq_schema =\u0026#39;.messages[].content\u0026#39; # will report an error Expected page_content is string, got \u0026lt;class \u0026#39; NoneType \u0026#39;\u0026gt; instead. page_content =False, # add this line after error reporting) data = loader.load () print( data[ 0]) PDF Usage\n\u0026#39;\u0026#39;\u0026#39; first usage \u0026#39;\u0026#39;\u0026#39; from langchain.document_loaders import PyPDFLoader _ loader = PyPDFLoader (\u0026#34;../examples/layout-parser-paper.pdf\u0026#34;) pages = loader.load_and_split ( ) print( pages[ 0]) \u0026#39;\u0026#39;\u0026#39; second usage \u0026#39;\u0026#39;\u0026#39; from langchain.document_loaders import MathpixPDFLoader _ loader = MathpixPDFLoader (\u0026#34; example_data /layout-parser-paper.pdf\u0026#34;) data = loader.load () print( data[ 0]) \u0026#39;\u0026#39;\u0026#39; third usage \u0026#39;\u0026#39;\u0026#39; from langchain.document_loaders import UnstructuredPDFLoader _ loader = UnstructuredPDFLoader (\u0026#34;../examples/layout-parser-paper.pdf\u0026#34;) data = loader.load () print( data[ 0]) data connection - document conversion Once files are loaded, adapting them to suit specific applications becomes paramount. A classic scenario involves segmenting a lengthy document into smaller fragments to align with your model\u0026rsquo;s context window. LangChain is equipped with an array of integrated document conversion utilities. These tools facilitate effortless operations such as splitting, merging, filtering, and various other document manipulations.\nText segmentation by character\nstate_of_the_union = \u0026#34;\u0026#34;\u0026#34; Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \u0026#34;\u0026#34;\u0026#34; from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter ( separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 128, # Chunk length chunk_overlap = 10, # Overlapped text length length_function = len , ) texts = text_ splitter. create _documents ([ state_of_the_union ]) print( texts[ 0]) # Here metadatas are used to distinguish different documents metadatas = [{\u0026#34;document\u0026#34;: 1}, {\u0026#34;document\u0026#34;: 2}] documents = text_ splitter. create _documents ([ state_of_the_union , state_of_the_union ], metadatas = metadatas ) pprint (documents) # Get the cut text print( text_ splitter. split _text ( state_of_the_union )[0]) Split the code\nfrom langchain.text_splitter import ( RecursiveCharacterTextSplitter , Language, ) print([ e.value for e in Language]) # supported languages print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)) # separator PYTHON_CODE = \u0026#34;\u0026#34;\u0026#34; def hello_world ( ) : print( \u0026#34;Hello, World!\u0026#34;) # Call the function hello_world ( ) _ \u0026#34;\u0026#34;\u0026#34; python_splitter = RecursiveCharacterTextSplitter.from_language ( _ language= Language.PYTHON , chunk_size =50, chunk_overlap =0 ) python_docs = python_ splitter. create _documents ([PYTHON_CODE]) python_docs \u0026#34;\u0026#34;\u0026#34; [ Document( page_content =\u0026#39;def hello_world ():\\n print(\u0026#34;Hello, World!\u0026#34;)\u0026#39;, metadata={}), Document( page_content =\u0026#39;# Call the function\\ nhello_world ()\u0026#39;, metadata={})] \u0026#34;\u0026#34;\u0026#34; Split by markdownheader\nFor example: md = # Foo\\n\\n ## Bar\\n\\ nHi this is Jim \\ nHi this is Joe\\n\\n ## Baz\\n\\n Hi this is Molly' .We define the split header : [(\u0026quot;#\u0026quot;, \u0026quot;Header 1\u0026quot;),(\u0026quot;##\u0026quot;, \u0026quot;Header 2\u0026quot;)] The text should be split by the common header, and finally get: `{\u0026lsquo;content\u0026rsquo;: \u0026lsquo;Hi this is Jim \\nHi this is Joe\u0026rsquo;, \u0026lsquo;metadata\u0026rsquo;: {\u0026lsquo;Header 1\u0026rsquo;: \u0026lsquo;Foo\u0026rsquo;, \u0026lsquo;Header 2\u0026rsquo;: \u0026lsquo;Bar\u0026rsquo;}} {\u0026lsquo;content\u0026rsquo;: \u0026lsquo;Hi this is Molly\u0026rsquo;, \u0026lsquo;metadata\u0026rsquo;: {\u0026rsquo; Header 1\u0026rsquo;: \u0026lsquo;Foo\u0026rsquo;, \u0026lsquo;Header 2\u0026rsquo;: \u0026lsquo;Baz\u0026rsquo;}}\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter markdown_document = \u0026#34;# Foo\\n\\n ## Bar\\n\\ nHi this is Jim\\n\\ nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\ n\\n Hi this is Molly\u0026#34; headers_to_split_on = [ (\u0026#34;#\u0026#34;, \u0026#34;Header 1\u0026#34;), (\u0026#34;##\u0026#34;, \u0026#34;Header 2\u0026#34;), (\u0026#34;###\u0026#34;, \u0026#34;Header 3\u0026#34;), ] markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on) md_header_splits = markdown_ splitter. split _text ( markdown_document ) md_header_splits \u0026#34;\u0026#34;\u0026#34; [ Document( page_content =\u0026#39;Hi this is Jim \\ nHi this is Joe\u0026#39;, metadata={\u0026#39;Header 1\u0026#39;: \u0026#39;Foo\u0026#39;, \u0026#39;Header 2\u0026#39;: \u0026#39;Bar\u0026#39;}), Document( page_content =\u0026#39;Hi this is Lance\u0026#39;, metadata={\u0026#39;Header 1\u0026#39;: \u0026#39;Foo\u0026#39;, \u0026#39;Header 2\u0026#39;: \u0026#39;Bar\u0026#39;, \u0026#39;Header 3\u0026#39;: \u0026#39;Boo\u0026#39;}), Document( page_content =\u0026#39;Hi this is Molly\u0026#39;, metadata={\u0026#39;Header 1\u0026#39;: \u0026#39;Foo\u0026#39;, \u0026#39;Header 2\u0026#39;: \u0026#39;Baz\u0026#39;})] \u0026#34;\u0026#34;\u0026#34; Model I/O Language models come in various forms, including Large Language Models (LLMs), chat models, and text embedding models.\nLarge Language Models (LLMs): These are the primary models we discuss. They take a text string as input and produce a text string as an output.\nChat Models: These are our second focus. While they are typically powered by a language model, their API is more structured. Specifically, they process a sequence of chat messages as input and generate a corresponding chat message in response.\nText Embedding Models: These models are designed to convert text into a numerical representation. When given text as input, they yield a list of floating-point numbers.\nLangChain offers essential tools to seamlessly integrate any language model.\nTIP: Templating, Dynamic Selection, and Managing Model Input A new way to program models is through prompts. A hint refers to the input to the model. This input is usually composed of multiple components. LangChain provides several classes and functions that make it easy to construct and process hint messages. Commonly used methods are: PromptTemplate: where parameterizes model input; and Example Selector: dynamically selects examples to be included in the prompt.\nA prompt template can contain: instructions to the language model; a small set of examples to help the language model generate a better response; a question to the language model. For example:\nfrom langchain import PromptTemplate template = \u0026#34;\u0026#34;\u0026#34;/ You are a naming consultant for new companies. What is a good name for a company that makes {product}? \u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate. from_template (template) prompt. format (product=\u0026#34;colorful socks\u0026#34;) \u0026#34;\u0026#34;\u0026#34; You are a naming consultant for new companies. What is a good name for a company that makes colorful socks? \u0026#34;\u0026#34;\u0026#34; If you need to create a role-related message template, you need to use MessagePromptTemplate .\ntemplate=\u0026#34;You are a helpful assistant that translates { input_language } to { output_language }.\u0026#34; system_message_prompt = SystemMessagePromptTemplate.from_template (template) human_template =\u0026#34;{text}\u0026#34; human_message_prompt = HumanMessagePromptTemplate. from_template ( human_template ) Types of MessagePromptTemplate : LangChain provides different types of MessagePromptTemplate . The most commonly used ones are AIMessagePromptTemplate , SystemMessagePromptTemplate , and HumanMessagePromptTemplate , which create AI messages, system messages, and human messages, respectively.\nGeneric Prompt Template: Suppose we want LLM to generate an English interpretation of a function name. To accomplish this, we\u0026rsquo;ll create a custom prompt template that takes a function name as input and format the prompt template to provide the source code for that function.\nWe first create a function that will return the source code of the given function.\nimport inspect def get_source_code ( function_name ): # Get the source code of the function return inspect. getsource ( function_name ) Next, we\u0026rsquo;ll create a custom prompt template that takes the function name as input and formats the prompt template to provide the function\u0026rsquo;s source code.\nfrom langchain.prompts import StringPromptTemplate from pydantic import BaseModel , validator class FunctionExplainerPromptTemplate ( StringPromptTemplate , BaseModel ): \u0026#34;\u0026#34;\u0026#34;A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\u0026#34;\u0026#34;\u0026#34; @validator(\u0026#34;input_variables\u0026#34;) def validate_input_variables ( cls , v): \u0026#34;\u0026#34;\u0026#34;Validate that the input variables are correct.\u0026#34;\u0026#34;\u0026#34; if len (v ) ! = 1 or \u0026#34; function_name \u0026#34; not in v: raise ValueError ( \u0026#34; function_name must be the only input_variable .\u0026#34;) return v def format( self, ** kwargs ) -\u0026gt; str: # Get the source code of the function source_code = get_source_code ( kwargs [\u0026#34; function_name \u0026#34;]) # Generate the prompt to be sent to the language model prompt = f\u0026#34;\u0026#34;\u0026#34; Given the function name and source code, generate an English language explanation of the function. Function Name: { kwargs [\u0026#34; function_name \u0026#34; ]._ _name__} Source Code: { source_code } Explanation: \u0026#34;\u0026#34;\u0026#34; return prompt def_prompt_type ( self): return \u0026#34;function-explainer\u0026#34; Language Model: Interface for Language Models Language models vary, with notable distinctions between LLMs and chat models. In LangChain, LLM denotes a plain text completion model. These models have APIs that accept a string prompt and return a string completion. For instance, OpenAI\u0026rsquo;s GPT-3 is an LLM. Chat models, while often underpinned by LLMs, are tailored for conversations. Their API differs from the plain text completion model, accepting a list of chat messages, tagged with the speaker (typically \u0026ldquo;system\u0026rdquo;, \u0026ldquo;AI\u0026rdquo;, or \u0026ldquo;human\u0026rdquo;), and returning an \u0026ldquo;AI\u0026rdquo; chat message. Both GPT-4 and Anthropic\u0026rsquo;s Claude are chat models.\nTo facilitate the interchangeability of LLM and chat models, both adopt the foundational language model interface, revealing common methods like \u0026ldquo;predict\u0026rdquo; and \u0026ldquo;pred messages\u0026rdquo;. While it\u0026rsquo;s advisable to use model-specific methods (e.g., LLM\u0026rsquo;s \u0026ldquo;predict\u0026rdquo; and chat model\u0026rsquo;s \u0026ldquo;predict message\u0026rdquo;), a shared interface is beneficial for versatile applications.\nUsing an LLM is straightforward: input a string and receive a string completion.\nThe generate function allows batch calls and richer outputs. For instance:\nllm_result = llm. generate ([\u0026#34;Tell me a joke\u0026#34;, \u0026#34;Tell me a poem\u0026#34;]*15) len ( llm_result.generations ) _ \u0026#39;\u0026#39;\u0026#39; 30 \u0026#39;\u0026#39;\u0026#39; llm_result . generations [0] \u0026#39;\u0026#39;\u0026#39; [ Generation( text=\u0026#39;\\n\\ nWhy did the chicken cross the road?\\n\\ nTo get to the other side!\u0026#39;), Generation( text=\u0026#39;\\n\\ nWhy did the chicken cross the road?\\n\\ nTo get to the other side.\u0026#39;)] \u0026#39;\u0026#39;\u0026#39; llm_result.generations [-1 ] \u0026#39;\u0026#39;\u0026#39; [Generation(text=\u0026#34;\\n\\ nWhat if love neverspeech \\n\\ nWhat if love never ended\\n\\ nWhat if love was only a feeling\\n\\ nI\u0026#39;ll never know this love\\n\\ nIt\u0026#39;s not a feeling\\n \\ nBut it\u0026#39;s what we have for each other\\n\\ nWe just know that love is something strong\\n\\ nAnd we can\u0026#39;t help but be happy\\n\\ nWe just feel what love is for us\\n\\ nAnd we love each other with all our heart\\n\\ nWe just don\u0026#39;t know how\\n\\ nHow it will go\\n\\ nBut we know that love is something strong\\n\\ nAnd we\u0026#39;ll always have each other\\n\\ nIn our lives .\u0026#34;), Generation( text=\u0026#39;\\n\\ nOnce upon a time\\n\\ nThere was a love so pure and true\\n\\ nIt lasted for centuries\\n\\ nAnd never became stale or dry\\n\\ nIt was moving and alive\\n\\ nAnd the heart of the love-ick\\n\\ nIs still beating strong and true.\u0026#39;)] You can also access provider specific information that is returned. This information is NOT standardized across providers. \u0026#39;\u0026#39;\u0026#39; llm_result.llm_output \u0026#39;\u0026#39;\u0026#39; {\u0026#39; token_usage \u0026#39;: {\u0026#39; completion_tokens \u0026#39;: 3903, \u0026#39; total_tokens \u0026#39;: 4023 , \u0026#39; prompt_tokens \u0026#39; : 120}} \u0026#39;\u0026#39;\u0026#39; Output Analyzer: Deciphering Model Outputs Output parsers structure language model responses. They primarily implement two methods:\n\u0026ldquo;Get format specification\u0026rdquo;: This method returns a string containing an indication of how the output of the language model should be formatted. \u0026ldquo;parse\u0026rdquo;: This method takes a string (assumed to be a response from a language model) and parses it into some structure. \u0026ldquo;parsing with hints\u0026rdquo;: This method is optional and takes a string (supposed to be a response from the language model) and a hint (supposed to be the hint that produced such a response) and parses it into some structure . Hints are primarily provided in cases where the OutputParser wants to retry or fix the output in some way, and needs information from the hint to do so. from langchain.prompts import PromptTemplate , ChatPromptTemplate , HumanMessagePromptTemplate from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI _ from langchain.output_parsers import PydanticOutputParser _ from pydantic import BaseModel , Field, validator from typing import list model_name = \u0026#39;text-davinci-003\u0026#39; temperature = 0.0 model = OpenAI( model_name = model_name , temperature=temperature) # Define your desired data structure. class Joke( BaseModel ): setup: str = Field( description=\u0026#34;question to set up a joke\u0026#34;) punchline: str = Field( description=\u0026#34;answer to resolve the joke\u0026#34;) # You can add custom validation logic easily with Pydantic . @validator(\u0026#39;setup\u0026#39;) def question_ends_with_question_mark ( cls , field) : if field[ -1] != \u0026#39;?\u0026#39;: raise ValueError ( \u0026#34;Badly formed question!\u0026#34;) return field # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser ( pydantic_object =Joke) prompt = PromptTemplate ( template=\u0026#34;Answer the user query.\\n{ format_ instructions }\\ n{query}\\n\u0026#34;, input_variables = [\u0026#34;query\u0026#34;], partial_variables ={ \u0026#34; format_instructions \u0026#34;: parser.get_format_instructions ()} ) # And a query intended to prompt a language model to populate the data structure. joke_query = \u0026#34;Tell me a joke.\u0026#34; _input = prompt. format _prompt (query= joke_query ) output = model(_ input. to_ string ( )) parser. parse (output) Joke( setup=\u0026#39;Why did the chicken cross the road?\u0026#39;, punchline=\u0026#39;To get to the other side!\u0026#39;) Chain components For intricate applications that necessitate linking LLMs in series, the Chain component is essential. LangChain offers a Chain interface for such applications. Its primary interface is:\nclass Chain( BaseModel , ABC): \u0026#34;\u0026#34;\u0026#34;Base interface that all chains should implement.\u0026#34;\u0026#34;\u0026#34; memory: BaseMemory callbacks: Callbacks def __call_ _( self, inputs: Any, return_only_outputs : bool = False, callbacks: Callbacks = None, ) -\u0026gt; Dict [ str, Any]: ... Chains allow us to join multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate , and passes the formatted response to LLM. We can build more complex chains by combining multiple chains together, or combining chains with other components. LLMChain is the most basic building block chain. It takes a prompt template, formats it with user input, and returns a response from the LLM.\nWe start by creating a prompt template:\nfrom langchain.llms import OpenAI from langchain.prompts import PromptTemplate llm = OpenAI(temperature=0.9) prompt = PromptTemplate ( input_variables = [\u0026#34;product\u0026#34;], template=\u0026#34;What is a good name for a company that makes {product}?\u0026#34;, ) Then, create a very simple chain that will take input from the user, use it to format the prompt, and send it to LLM.\nfrom langchain.chains import LLMChain chain = LLMChain ( llm = llm , prompt=prompt) # Run the chain only specifying the input variable. print( chain. run (\u0026#34;colorful socks\u0026#34;)) Colorful Toes Co. \u0026#39;\u0026#39;\u0026#39; If you have multiple variables, you can use a dictionary to enter them all at once. \u0026#39;\u0026#39;\u0026#39; prompt = PromptTemplate ( input_variables = [ \u0026#34;company\u0026#34;, \u0026#34;product\u0026#34;], template=\u0026#34;What is a good name for {company} that makes {product}?\u0026#34;, ) chain = LLMChain ( llm = llm , prompt=prompt) print( chain. run ({ \u0026#39;company\u0026#39;: \u0026#34;ABC Startup\u0026#34;, \u0026#39;product\u0026#39;: \u0026#34;colorful socks\u0026#34; })) Socktopia Colorful Creations. It is also possible to use a chat model in LLMChain :\nfrom langchain.chat_models import ChatOpenAI _ from langchain.prompts.chat import ( ChatPromptTemplate , HumanMessagePromptTemplate , ) human_message_prompt = HumanMessagePromptTemplate ( prompt = PromptTemplate ( template=\u0026#34;What is a good name for a company that makes {product}?\u0026#34;, input_variables = [\u0026#34;product\u0026#34;], ) ) chat_prompt_template = ChatPromptTemplate.from_messages ([ human_message_prompt ]) chat = ChatOpenAI (temperature=0.9) chain = LLMChain ( llm =chat, prompt= chat_prompt_template ) print( chain. run (\u0026#34;colorful socks\u0026#34;)) Rainbow Socks Co. Chain Serialization For saving and loading chains, refer to: Serialization | ️ Langchain .\nIn Summary LangChain offers a suite of components tailored for diverse applications like personal assistants, chatbots, and more. It streamlines the development of advanced language model applications through its modular design. By grasping core concepts like components, chains, and output parsers, users can craft custom solutions for specific needs. LangChain empowers users to harness the full capabilities of language models, paving the way for intelligent, context-aware applications across various domains.\n"}]